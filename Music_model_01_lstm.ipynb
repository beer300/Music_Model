{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNwWfRsfXNOdcmWuusgovou",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beer300/Music_Model/blob/main/Music_model_01_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLoPvBzlOK-I",
        "outputId": "47926b9a-ade5-4ba2-d23e-9e18480476fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (2.5.0.post0)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.10.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.6.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (0.11.9)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "BATCH_SIZE = 8\n",
        "data_dir = '/content/drive/My Drive/data/spectogram_30'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMTuswlFOZXm",
        "outputId": "591c9af3-a602-414c-ce27-b0cd67d4514a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qfn9qjRDOcoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class SpectrogramDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = [\n",
        "            os.path.join(image_dir, fname)\n",
        "            for fname in os.listdir(image_dir)\n",
        "            if fname.endswith('.png')\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        img = Image.open(img_path)  # Removed .convert(\"L\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "full_dataset = SpectrogramDataset(image_dir=data_dir, transform=transform)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "metadata": {
        "id": "pIzh37wCwyQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=4):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 1, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.model(x)\n",
        "\n",
        "        return out.view(-1, 1)"
      ],
      "metadata": {
        "id": "RdkXjqTNwy4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim=400, hidden_dim=512, num_layers=4):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # Add an LSTM to process latent vectors\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=latent_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Fully connected layer to reshape LSTM output for convolutional layers\n",
        "        self.fc = nn.Linear(hidden_dim, 1024 * 2)  # Adjust as needed for your architecture\n",
        "        self.main = nn.Sequential(\n",
        "\n",
        "            nn.ConvTranspose2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear'),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear'),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.5),\n",
        "        )\n",
        "        self.final = nn.Sequential(\n",
        "\n",
        "            nn.Upsample(scale_factor=4, mode='bilinear'),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(16, 4, kernel_size=3, stride=1, padding=0),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, z):\n",
        "\n",
        "        z = z.view(-1, 1, self.latent_dim)  # Reshape for LSTM (batch, seq_len, input_dim)\n",
        "        #print(f\"z\", z.shape)\n",
        "        lstm_out, _ = self.lstm(z)\n",
        "        #print(f\"lstm_out\", lstm_out.shape)\n",
        "        lstm_out = lstm_out[:, -1, :]  # Take the last time step output\n",
        "        #print(f\"lstm_out\", lstm_out.shape)\n",
        "        x = self.fc(lstm_out)\n",
        "        #print(f\"x\", x.shape)\n",
        "        x = x.view(-1, 1024, 1, 2)  # Reshape to feed into ConvTranspose layers\n",
        "        #print(f\"x\", x.shape)\n",
        "        x = self.main(x)\n",
        "        #print(f\"x\", x.shape)\n",
        "        x = self.final(x)\n",
        "        #print(f\"x\", x.shape)\n",
        "        x = nn.functional.interpolate(x, size=(1164, 2364), mode='bilinear', align_corners=False)\n",
        "        #print(f\"x\", x.shape)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "jG0AC9qHw5HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GAN(pl.LightningModule):\n",
        "    def _write_disk_spectrogram(self, path, dpi=120):\n",
        "        plt.savefig(path, dpi=dpi, bbox_inches='tight', pad_inches=0)\n",
        "        plt.close()\n",
        "    def __init__(self, latent_dim=100, lr=0.0002, sample_rate=48000):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.automatic_optimization = False\n",
        "        self.generator = Generator(latent_dim=self.hparams.latent_dim)\n",
        "        self.discriminator = Discriminator(in_channels=4)\n",
        "        self.validation_z = torch.randn(1, self.hparams.latent_dim, 1, 2)\n",
        "    def forward(self, z):\n",
        "        return self.generator(z)\n",
        "    def adversarial_loss(self, y_hat, y):\n",
        "        return F.binary_cross_entropy_with_logits(y_hat, y)\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        opt_d, opt_g = self.optimizers()\n",
        "        real_imgs = batch\n",
        "        z = torch.randn(real_imgs.size(0), self.hparams.latent_dim, 1, 2).type_as(real_imgs)\n",
        "        fake_imgs = self(z).detach()\n",
        "        y_hat_real = self.discriminator(real_imgs)\n",
        "        y_hat_fake = self.discriminator(fake_imgs)\n",
        "        real_loss = self.adversarial_loss(y_hat_real, torch.ones_like(y_hat_real))\n",
        "        fake_loss = self.adversarial_loss(y_hat_fake, torch.zeros_like(y_hat_fake))\n",
        "        d_loss = 0.5 * (real_loss + fake_loss)\n",
        "        self.manual_backward(d_loss)\n",
        "        opt_d.step()\n",
        "        opt_d.zero_grad()\n",
        "        z = torch.randn(real_imgs.size(0), self.hparams.latent_dim).type_as(real_imgs)\n",
        "        fake_imgs = self(z)\n",
        "        y_hat = self.discriminator(fake_imgs)\n",
        "        g_loss = self.adversarial_loss(y_hat, torch.ones_like(y_hat))\n",
        "        self.manual_backward(g_loss)\n",
        "        opt_g.step()\n",
        "        opt_g.zero_grad()\n",
        "        self.log('d_loss', d_loss, prog_bar=True, on_epoch=True)\n",
        "        self.log('g_loss', g_loss, prog_bar=True, on_epoch=True)\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        real_imgs = batch\n",
        "        z = torch.randn(real_imgs.size(0), self.hparams.latent_dim).type_as(real_imgs)\n",
        "        fake_imgs = self(z)\n",
        "        y_hat_real = self.discriminator(real_imgs)\n",
        "        y_hat_fake = self.discriminator(fake_imgs)\n",
        "        real_loss = self.adversarial_loss(y_hat_real, torch.ones_like(y_hat_real))\n",
        "        fake_loss = self.adversarial_loss(y_hat_fake, torch.zeros_like(y_hat_fake))\n",
        "        d_loss = 0.5 * (real_loss + fake_loss)\n",
        "        y_hat = self.discriminator(fake_imgs)\n",
        "        g_loss = self.adversarial_loss(y_hat, torch.ones_like(y_hat))\n",
        "        self.log('val_d_loss', d_loss, prog_bar=True, on_epoch=True)\n",
        "        self.log('val_g_loss', g_loss, prog_bar=True, on_epoch=True)\n",
        "    def configure_optimizers(self):\n",
        "        lr = self.hparams.lr\n",
        "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "        return [opt_d, opt_g]\n",
        "    def on_validation_epoch_end(self):\n",
        "        z = self.validation_z.type_as(self.generator.main[0].weight)\n",
        "        spec = self.generator(z)[0, 0].detach().cpu().numpy()\n",
        "        spec = self._center_crop_or_pad(spec, target_h=1164, target_w=2364)\n",
        "\n",
        "        plt.figure(figsize=(16, 8))\n",
        "        plt.imshow(librosa.amplitude_to_db(spec, ref=np.max), cmap='inferno', origin='lower')\n",
        "        plt.colorbar(format='%+2.0f dB')\n",
        "        plt.title(f'Spectrogram - Epoch {self.current_epoch}')\n",
        "        plt.tight_layout()\n",
        "        png_old_path = os.path.join('/content/drive/My Drive/data/generated_old_spectrograms', f\"epoch{self.current_epoch}_old_spectrogram.png\")\n",
        "        plt.savefig(png_old_path, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "        plt.figure(figsize=(20, 10))  # Changed from (16, 8) to match audio class\n",
        "        librosa.display.specshow(spec, sr=self.hparams.sample_rate, hop_length=1024, x_axis='time', y_axis='log')\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        png_path = os.path.join('/content/drive/My Drive/data/generated_spectrograms', f\"epoch{self.current_epoch}_spectrogram.png\")\n",
        "        self._write_disk_spectrogram(png_path)\n",
        "\n",
        "        audio = librosa.griffinlim(\n",
        "            S=spec,\n",
        "            n_iter=64,\n",
        "            hop_length=1024,\n",
        "            win_length=1024,\n",
        "            center=True\n",
        "        )\n",
        "        sr = self.hparams.sample_rate\n",
        "        os.makedirs(\"generated_audio\", exist_ok=True)\n",
        "        wav_path = os.path.join('/content/drive/My Drive/data/generated_audio', f\"epoch{self.current_epoch}_sample.wav\")\n",
        "        sf.write(wav_path, audio, sr)\n",
        "\n",
        "        print(f\"[INFO] Saved spectrogram to {png_path}\")\n",
        "        print(f\"[INFO] Saved audio to {wav_path}\")\n",
        "    def _center_crop_or_pad(self, spec: np.ndarray, target_h: int, target_w: int) -> np.ndarray:\n",
        "        h, w = spec.shape\n",
        "        if h > target_h:\n",
        "            diff = h - target_h\n",
        "            spec = spec[diff // 2 : diff // 2 + target_h, :]\n",
        "        if w > target_w:\n",
        "            diff = w - target_w\n",
        "            spec = spec[:, diff // 2 : diff // 2 + target_w]\n",
        "        h, w = spec.shape\n",
        "        if h < target_h:\n",
        "            diff = target_h - h\n",
        "            top_pad = diff // 2\n",
        "            bot_pad = diff - top_pad\n",
        "            spec = np.pad(spec, ((top_pad, bot_pad), (0, 0)), mode=\"constant\", constant_values=0)\n",
        "        if w < target_w:\n",
        "            diff = target_w - w\n",
        "            left_pad = diff // 2\n",
        "            right_pad = diff - left_pad\n",
        "            spec = np.pad(spec, ((0, 0), (left_pad, right_pad)), mode=\"constant\", constant_values=0)\n",
        "        return spec\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import pytorch_lightning as pl\n",
        "    from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    torch.manual_seed(42)\n",
        "    model = GAN(\n",
        "        latent_dim=100,\n",
        "        lr=0.001,\n",
        "        sample_rate=48000\n",
        "    )\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        dirpath=\"checkpoints\",\n",
        "        filename=\"gan-{epoch:02d}-{val_g_loss:.2f}\",\n",
        "        save_top_k=-1,\n",
        "        verbose=True,\n",
        "        monitor=\"val_g_loss\",\n",
        "        mode=\"min\",\n",
        "        save_weights_only=True,\n",
        "        every_n_epochs=1\n",
        "    )\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=200,\n",
        "        accelerator=\"gpu\",\n",
        "        devices=1,\n",
        "        precision=16,\n",
        "        callbacks=[checkpoint_callback]\n",
        "    )\n",
        "    trainer.fit(\n",
        "        model,\n",
        "        train_dataloaders=train_dataloader,\n",
        "        val_dataloaders=val_dataloader\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "god9XdTew7mJ",
        "outputId": "fbc6de35-2511-443c-fee4-e1c742825309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'mixed' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-6f7a0ba7342f>\u001b[0m in \u001b[0;36m<cell line: 114>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0maccelerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpu\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mdevices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmixed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mixed' is not defined"
          ]
        }
      ]
    }
  ]
}